from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
import os
import wget
import time

options = webdriver.ChromeOptions()
options.add_argument('--disable-notifications')

driver = webdriver.Chrome(options=options)

# Open the webpage
driver.get("http://www.facebook.com")

# Target username
username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[name='email']")))
password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[name='pass']")))

# Enter username and password
username.clear()
username.send_keys("8552851976")
password.clear()
password.send_keys("Mad#1504")

# Target the login button and click it
button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button[type='submit']"))).click()

# We are logged in!
time.sleep(5)

images = []
captions = []

# Iterate over both uploaded and tagged images respectively
for i in ["Your_Photos", "Albums"]:
    driver.get("https://www.facebook.com/profile.php?id=100077663996875" + i + "/")
    time.sleep(5)

    # Scroll down
    for j in range(0, 1):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(10)

    # Target all the link elements on the page
    anchors = driver.find_elements(By.TAG_NAME, 'a')
    anchors = [a.get_attribute('href') for a in anchors]
    # Narrow down all links to image links only
    anchors = [a for a in anchors if str(a).startswith("https://www.facebook.com/photo")]

    print('Found ' + str(len(anchors)) + ' links to images')

    # Extract the [1]st image element in each link
    for a in anchors:
        driver.get(a)  # Navigate to link
        time.sleep(5)  # Wait a bit
        img = driver.find_elements(By.TAG_NAME, "img")
        if img:
            images.append(img[0].get_attribute("src"))  # Use img[0] instead of img[1]
        else:
            images.append("")  # If no image, append an empty string

        # Extract the caption
        caption_elements = driver.find_elements(By.XPATH, '//*[@id="mount_0_0_oy"]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div/div[2]/div/div/div/div[1]/div[1]/div[1]/div[2]/span')
        if caption_elements:
            captions.append(caption_elements[0].text)
        else:
            captions.append("")  # If no caption, append an empty string

             if img:
            images.append(img[0].get_attribute("src"))  # Use img[0] instead of img[1]
        else:
            images.append("")  # If no image, append an empty string

        # Extract the caption
        caption_elements = driver.find_elements(By.XPATH, '//*[@id="mount_0_0_oy"]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div/div[2]/div/div/div/div[1]/div[1]/div[1]/div[2]/span')
        if caption_elements:
            captions.append(caption_elements[0].text)
        else:
            captions.append("")  # If no caption, append an empty string

print('I scraped ' + str(len(images)) + ' images!')
print('I scraped ' + str(len(captions)) + ' captions!')

# Create a directory to store the images and captions
path = os.getcwd()
path = os.path.join(path, "FB_SCRAPED")
os.mkdir(path)

# Download images
counter = 0
for image in images:
    save_as = os.path.join(path, str(counter) + '.jpg')
    wget.download(image, save_as)
    counter += 1

# Save captions to text files
for i, caption in enumerate(captions):
    with open(os.path.join(path, f"caption_{i+1}.txt"), "w") as f:
        f.write(caption)
    print(f"Caption {i+1} saved to {os.path.join(path, f'caption_{i+1}.txt')}")
